{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_db(file,file_type):\n",
    "    '''\n",
    "    This module reads and prepares the db : cleans,\n",
    "    deletes the columns with 'Unnamed' values,\n",
    "    fills NaN values\n",
    "    input : \n",
    "    file # the movieset file\n",
    "    file type # csv file type\n",
    "    output :\n",
    "    df # read movie set dataframe \n",
    "    '''\n",
    "    if file_type == 'csv':\n",
    "        df = pd.read_csv(file, sep=';', dtype=str) # Read CSV File    \n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df = df.fillna('') # clean the data - get rid of NaN\n",
    "    df = df.astype(str) # change the default data type to string\n",
    "    return(df)\n",
    "\n",
    "def combine_row_text(features,df):\n",
    "    '''\n",
    "    This module combines text columns and sets the case of the combined text to lower\n",
    "    args :\n",
    "    input : \n",
    "    features # list of database columns to search in the movies database\n",
    "    df # pandas dataframe\n",
    "    output : counts # dataframe\n",
    "    '''\n",
    "    df['comb'] = ''\n",
    "    for feature  in features:\n",
    "        df['comb']= df['comb'].str.cat(df[feature].copy(), sep =',')\n",
    "    df['comb'] = df['comb'].str.lower()\n",
    "    return(df)\n",
    "\n",
    "def highest_count_words(df):\n",
    "    '''\n",
    "    This module takes as input the imported movie dataset \n",
    "    and counts the occurance of the words and\n",
    "    sets the columns in decreasing order\n",
    "    args :\n",
    "    input : df # pandas dataframe of the imported movie dataset\n",
    "    output : counts # dataframe of the counted words\n",
    "    '''\n",
    "    \n",
    "    count_matrix = vectorizer.fit_transform(df.keywords)\n",
    "    count_matrix = count_matrix.astype(np.int32)\n",
    "    counts = pd.DataFrame(count_matrix.toarray(),\n",
    "                      index=df.title,\n",
    "                      columns=vectorizer.get_feature_names())\n",
    "    return(counts)\n",
    "\n",
    "def find_delete_columns(counts,character,char): \n",
    "    '''\n",
    "    This module is run off of the delete columns mega\n",
    "    and deletes the unwanted columns of words\n",
    "    args :\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    character # regex code for the particular character(s)\n",
    "    char # the unwanted character(s) that I want deleted\n",
    "    output :\n",
    "    counts # dataframe without the unwanted characters\n",
    "    '''\n",
    "    t = 0 # column counter\n",
    "    n = 1\n",
    "    orig = len(counts.columns)\n",
    "    #print(' ')\n",
    "    #print(' ')\n",
    "    #print(f'Original column count is {len(counts.columns)}')\n",
    "    print(f'removing {char} .... ')\n",
    "    for i in counts.columns:\n",
    "        if t == (n * 1000):\n",
    "            # print(str(round(t/len(counts.columns)*100,0))+'%')\n",
    "            n +=1\n",
    "        t+=1\n",
    "        if re.findall(character, i):\n",
    "            del counts[i]\n",
    "    #print(f'after deleting the unwanted word -{char}-, the list is {len(counts.columns)} long')\n",
    "    #print(f'Reduction effectiveness : {str(round((1-(len(counts.columns)/orig))*100,0))} %')\n",
    "    return(counts)\n",
    "\n",
    "def delete_columns_mega(counts):                   \n",
    "    '''\n",
    "    This module calls the sub module and controls the deletion of unwanted columns\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    output :\n",
    "    counts # dataframe without the unwanted characters    \n",
    "    '''\n",
    "    counts = find_delete_columns(counts, '\\d', 'any numeric')\n",
    "    counts = find_delete_columns(counts, '[\\uac00-\\ud7a3]', 'Korean characters')\n",
    "    counts = find_delete_columns(counts, '[\\u4e00-\\u9FFF]', 'Chinese characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0900-\\u097F]',' Indian characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0627-\\u064a]',' Arabic characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0400-\\u04FF]',' Russian characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0370-\\u03FF]',' Greek characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u30FF]',' Japanese Katagana characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u309F]',' Japanese Hiragana characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u30FF]',' Japanese Katagana characters')\n",
    "#     counts = find_delete_columns(counts, '(^has$)',' has')\n",
    "#     counts = find_delete_columns(counts, '(^time$)',' time')\n",
    "#     counts = find_delete_columns(counts, '(^never$)',' never')\n",
    "#     counts = find_delete_columns(counts, '(^director$)',' director')\n",
    "#     counts = find_delete_columns(counts, '(^john$)',' john')\n",
    "#     counts = find_delete_columns(counts, '(^and$)',' and')\n",
    "#     counts = find_delete_columns(counts, '(^will$)',' will')\n",
    "#     counts = find_delete_columns(counts, '(^you$)',' you')\n",
    "#     counts = find_delete_columns(counts, '(^they$)',' they')\n",
    "#     counts = find_delete_columns(counts, '(^is$)',' is')\n",
    "#     counts = find_delete_columns(counts, '(^film$)',' film')\n",
    "#     counts = find_delete_columns(counts, '(^it$)',' it')\n",
    "#     counts = find_delete_columns(counts, '(^one$)',' one')\n",
    "#     counts = find_delete_columns(counts, '(^its-?$)',' it,its')\n",
    "#     counts = find_delete_columns(counts, '(^of$)',' of')\n",
    "#     counts = find_delete_columns(counts, '(^on$)',' on')\n",
    "#     counts = find_delete_columns(counts, '(^for$)',' for')\n",
    "#     counts = find_delete_columns(counts, '(^the$)',' the')\n",
    "#     counts = find_delete_columns(counts, '(^to$)',' to')\n",
    "#     counts = find_delete_columns(counts, '(^your-?$)',' you,your')\n",
    "#     counts = find_delete_columns(counts, '(^new$)',' new')\n",
    "#     counts = find_delete_columns(counts, '(^man$)',' man')\n",
    "#     counts = find_delete_columns(counts, '(^woman$)',' woman')\n",
    "#     counts = find_delete_columns(counts, '(^no$)',' no')\n",
    "#     counts = find_delete_columns(counts, '(^his$)',' his')\n",
    "#     counts = find_delete_columns(counts, '(^he$)',' he')\n",
    "#     counts = find_delete_columns(counts, '(^hers$)',' hers')\n",
    "#     counts = find_delete_columns(counts, '(^can$)',' can')\n",
    "#     counts = find_delete_columns(counts, '(^all$)',' all')\n",
    "#     counts = find_delete_columns(counts, '(^from$)',' from')\n",
    "#     counts = find_delete_columns(counts, '(^are$)',' are')\n",
    "#     counts = find_delete_columns(counts, '(^there$)',' there')\n",
    "#     counts = find_delete_columns(counts, '(^an$)',' an')\n",
    "#     counts = find_delete_columns(counts, '(^with$)',' with')\n",
    "#     counts = find_delete_columns(counts, '(^in$)',' in')\n",
    "#     counts = find_delete_columns(counts, '(^be$)',' be')\n",
    "#     counts = find_delete_columns(counts, '(^what$)',' what')\n",
    "    return (counts)\n",
    "\n",
    "def most_frequent_words(counts):\n",
    "    '''\n",
    "    This module prepares the most frequent words of the \n",
    "    specific movie's selection types and ouputs to a csv\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    output :\n",
    "    counts_transposed # column transferred to rows\n",
    "    and saved to a csv\n",
    "    '''\n",
    "    counts_transposed = counts.T\n",
    "    counts_transposed['sum'] = counts_transposed.sum(axis=1)\n",
    "    cols = counts_transposed.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    counts_transposed = counts_transposed.astype(np.int32)\n",
    "    counts_transposed = counts_transposed[cols]\n",
    "    counts_transposed = counts_transposed.sort_values(by=['sum'], axis=0, ascending=False)\n",
    "    movie_counts = counts_transposed\n",
    "    most_frequent_words = counts_transposed[['sum']]\n",
    "    most_frequent_words = most_frequent_words.reset_index()\n",
    "    most_frequent_words = most_frequent_words.rename(columns={'index': 'word', 'sum':'frequency'}, index={'title': 'index'})\n",
    "    return(most_frequent_words,counts_transposed)\n",
    "\n",
    "def cloud_word(word,nr_of_movies,movie_counts):\n",
    "    var = word\n",
    "    df = movie_counts.T[[var]]\n",
    "    return(df.sort_values(var, ascending=False).head(nr_of_movies))\n",
    "\n",
    "def make_pandas(freq_words,nr_of_movies,movie_counts):\n",
    "    '''\n",
    "    '''\n",
    "    word_list = freq_words['word']\n",
    "    word_list = word_list.tolist()\n",
    "    t = 0\n",
    "    for i in word_list:\n",
    "        movies = cloud_word(i,nr_of_movies,movie_counts)\n",
    "        movies = movies[1:]\n",
    "        movies = movies.reset_index()\n",
    "        movies = pd.DataFrame(data=movies)\n",
    "        movies = movies.rename(columns={'title':i})\n",
    "        movies = movies.iloc[:, :-1]\n",
    "        if t == 0:\n",
    "            df1 = movies\n",
    "        elif t> 1:\n",
    "            df1 = pd.concat([df1,movies],axis=1)\n",
    "        t+=1\n",
    "    return(df1)\n",
    "        \n",
    "def word_cloud(wrds):\n",
    "    '''\n",
    "    The function is the word cloud generator\n",
    "    Input : colunms of title and keywords of words and\n",
    "    groups of words from the db.\n",
    "    Output : the word cloud graphic \n",
    "    '''\n",
    "    print(wrds)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    wordcloud = WordCloud(width = 500, height = 500, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords,\n",
    "                    random_state = None,\n",
    "                    min_font_size = 15).generate(','.join(wrds)) \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (10, 10), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(MAIN_FOLDER + MWML_FOLDER + 'word_cloud_graphic.png')\n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 23 fields in line 37, saw 24\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cc505af33257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mFILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"duplicates3.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAIN_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMWML_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#df = df[['title', 'director', 'tagline', 'genres', 'keywords']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bf52b4cad373>\u001b[0m in \u001b[0;36mprep_db\u001b[1;34m(file, file_type)\u001b[0m\n\u001b[0;32m     11\u001b[0m     '''\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'csv'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Read CSV File\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^Unnamed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clean the data - get rid of NaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_proj\\ml_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_proj\\ml_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_proj\\ml_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_proj\\ml_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 23 fields in line 37, saw 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import requests\n",
    "import re\n",
    "import psutil\n",
    "#import openpyxl\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "'''\n",
    "Imports the 44,000 movie database into a pandas df so you can retrieve the columns.\n",
    "There are 24 columns in the database ranging from keywords to director.\n",
    "'''\n",
    "# Load the dataframe\n",
    "MAIN_FOLDER = 'c:/Users/champ/Python_proj/'\n",
    "MWML_FOLDER = 'made_with_ml_repo/temp/moviebuddy/meta_data/'\n",
    "#FILE = \"movies_database_cleaned_from_made_with_ml.csv\"\n",
    "FILE = \"duplicates3.csv\"\n",
    "\n",
    "df = prep_db(MAIN_FOLDER + MWML_FOLDER + FILE,'csv')\n",
    "#df = df[['title', 'director', 'tagline', 'genres', 'keywords']]\n",
    "df = df[['title','keywords']]\n",
    "\n",
    "#df = df.head(38000)\n",
    "df['keywords'] = df['keywords'].str.replace(', ', ',')\n",
    "df['keywords'] = df['keywords'].str.replace('\\s', '_')\n",
    "print('The selection types are : ')\n",
    "print(df.info())\n",
    "\n",
    "print(f'RAM = {psutil.virtual_memory().percent}')\n",
    "\n",
    "# Main functions\n",
    "counts = highest_count_words(df) # counts all the individual words in the title and the keywords\n",
    "frequent_words,movie_counts = most_frequent_words(counts) # organizes the word count\n",
    "print(display(frequent_words.head(10)))\n",
    "movies = make_pandas(freq_words = frequent_words.head(50),nr_of_movies = 10,movie_counts = movie_counts)\n",
    "movies.to_csv (MAIN_FOLDER + MWML_FOLDER + 'word_cloud_movie_recommendations.csv', index = False, header=True, sep=';')\n",
    "\n",
    "# # This function cleans the database of unwanted words like \"the\" , \"an\" , other misc lanaguage characters, etc)  \n",
    "# counts = delete_columns_mega(counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A word cloud is generated here based on the occurance of keyword words\n",
    "'''\n",
    "#word_cloud(df['keywords'])\n",
    "word_cloud(df['keywords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
