{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_db(file,file_type):\n",
    "    '''\n",
    "    This module reads and prepares the db : cleans,\n",
    "    deletes the columns with 'Unnamed' values,\n",
    "    fills NaN values\n",
    "    input : \n",
    "    file # the movieset file\n",
    "    file type # csv file type\n",
    "    output :\n",
    "    df # read movie set dataframe \n",
    "    '''\n",
    "    if file_type == 'csv':\n",
    "        df = pd.read_csv(file, sep=',', dtype=str) # Read CSV File    \n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df = df.fillna('') # clean the data - get rid of NaN\n",
    "    df = df.astype(str) # change the default data type to string\n",
    "    return(df)\n",
    "\n",
    "def combine_row_text(features,df):\n",
    "    '''\n",
    "    This module combines text columns and sets the case of the combined text to lower\n",
    "    args :\n",
    "    input : \n",
    "    features # list of database columns to search in the movies database\n",
    "    df # pandas dataframe\n",
    "    output : counts # dataframe\n",
    "    '''\n",
    "    df['comb'] = ''\n",
    "    for feature  in features:\n",
    "        df['comb']= df['comb'].str.cat(df[feature].copy(), sep =',')\n",
    "    df['comb'] = df['comb'].str.lower()\n",
    "    return(df)\n",
    "\n",
    "def highest_count_words(df):\n",
    "    '''\n",
    "    This module takes as input the imported movie dataset \n",
    "    and counts the occurance of the words and\n",
    "    sets the columns in decreasing order\n",
    "    args :\n",
    "    input : df # pandas dataframe of the imported movie dataset\n",
    "    output : counts # dataframe of the counted words\n",
    "    '''\n",
    "    \n",
    "    count_matrix = vectorizer.fit_transform(df.keywords)\n",
    "    counts = pd.DataFrame(count_matrix.toarray(),\n",
    "                      index=df.title,\n",
    "                      columns=vectorizer.get_feature_names())\n",
    "    return(counts)\n",
    "\n",
    "def find_delete_columns(counts,character,char): \n",
    "    '''\n",
    "    This sub-module is run off of the delete columns mega\n",
    "    and deletes the unwanted columns of words\n",
    "    args :\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    character # regex code for the particular character(s)\n",
    "    char # the unwanted character(s) that I want deleted\n",
    "    output :\n",
    "    counts # dataframe without the unwanted characters\n",
    "    '''\n",
    "    t = 0 # column counter\n",
    "    n = 1\n",
    "    orig = len(counts.columns)\n",
    "    #print(' ')\n",
    "    #print(' ')\n",
    "    #print(f'Original column count is {len(counts.columns)}')\n",
    "    print(f'removing {char} .... ')\n",
    "    for i in counts.columns:\n",
    "        if t == (n * 1000):\n",
    "            # print(str(round(t/len(counts.columns)*100,0))+'%')\n",
    "            n +=1\n",
    "        t+=1\n",
    "        if re.findall(character, i):\n",
    "            del counts[i]\n",
    "    #print(f'after deleting the unwanted word -{char}-, the list is {len(counts.columns)} long')\n",
    "    #print(f'Reduction effectiveness : {str(round((1-(len(counts.columns)/orig))*100,0))} %')\n",
    "    return(counts)\n",
    "\n",
    "def delete_columns_mega(counts):                   \n",
    "    '''\n",
    "    This module calls the sub module and controls the deletion of unwanted columns\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    output :\n",
    "    counts # dataframe without the unwanted characters    \n",
    "    '''\n",
    "    counts = find_delete_columns(counts, '\\d', 'any numeric')\n",
    "    counts = find_delete_columns(counts, '[\\uac00-\\ud7a3]', 'Korean characters')\n",
    "    counts = find_delete_columns(counts, '[\\u4e00-\\u9FFF]', 'Chinese characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0900-\\u097F]',' Indian characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0627-\\u064a]',' Arabic characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0400-\\u04FF]',' Russian characters')\n",
    "    counts = find_delete_columns(counts, '[\\u0370-\\u03FF]',' Greek characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u30FF]',' Japanese Katagana characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u309F]',' Japanese Hiragana characters')\n",
    "    counts = find_delete_columns(counts, '[\\u3040-\\u30FF]',' Japanese Katagana characters')\n",
    "#     counts = find_delete_columns(counts, '(^has$)',' has')\n",
    "#     counts = find_delete_columns(counts, '(^time$)',' time')\n",
    "#     counts = find_delete_columns(counts, '(^never$)',' never')\n",
    "#     counts = find_delete_columns(counts, '(^director$)',' director')\n",
    "#     counts = find_delete_columns(counts, '(^john$)',' john')\n",
    "#     counts = find_delete_columns(counts, '(^and$)',' and')\n",
    "#     counts = find_delete_columns(counts, '(^will$)',' will')\n",
    "#     counts = find_delete_columns(counts, '(^you$)',' you')\n",
    "#     counts = find_delete_columns(counts, '(^they$)',' they')\n",
    "#     counts = find_delete_columns(counts, '(^is$)',' is')\n",
    "#     counts = find_delete_columns(counts, '(^film$)',' film')\n",
    "#     counts = find_delete_columns(counts, '(^it$)',' it')\n",
    "#     counts = find_delete_columns(counts, '(^one$)',' one')\n",
    "#     counts = find_delete_columns(counts, '(^its-?$)',' it,its')\n",
    "#     counts = find_delete_columns(counts, '(^of$)',' of')\n",
    "#     counts = find_delete_columns(counts, '(^on$)',' on')\n",
    "#     counts = find_delete_columns(counts, '(^for$)',' for')\n",
    "#     counts = find_delete_columns(counts, '(^the$)',' the')\n",
    "#     counts = find_delete_columns(counts, '(^to$)',' to')\n",
    "#     counts = find_delete_columns(counts, '(^your-?$)',' you,your')\n",
    "#     counts = find_delete_columns(counts, '(^new$)',' new')\n",
    "#     counts = find_delete_columns(counts, '(^man$)',' man')\n",
    "#     counts = find_delete_columns(counts, '(^woman$)',' woman')\n",
    "#     counts = find_delete_columns(counts, '(^no$)',' no')\n",
    "#     counts = find_delete_columns(counts, '(^his$)',' his')\n",
    "#     counts = find_delete_columns(counts, '(^he$)',' he')\n",
    "#     counts = find_delete_columns(counts, '(^hers$)',' hers')\n",
    "#     counts = find_delete_columns(counts, '(^can$)',' can')\n",
    "#     counts = find_delete_columns(counts, '(^all$)',' all')\n",
    "#     counts = find_delete_columns(counts, '(^from$)',' from')\n",
    "#     counts = find_delete_columns(counts, '(^are$)',' are')\n",
    "#     counts = find_delete_columns(counts, '(^there$)',' there')\n",
    "#     counts = find_delete_columns(counts, '(^an$)',' an')\n",
    "#     counts = find_delete_columns(counts, '(^with$)',' with')\n",
    "#     counts = find_delete_columns(counts, '(^in$)',' in')\n",
    "#     counts = find_delete_columns(counts, '(^be$)',' be')\n",
    "#     counts = find_delete_columns(counts, '(^what$)',' what')\n",
    "    return (counts)\n",
    "\n",
    "def most_frequent_words(counts):\n",
    "    '''\n",
    "    This function prepares the most frequent words of the \n",
    "    specific movie's selection types and ouputs to a csv\n",
    "    input : \n",
    "    counts # dataframe of column of counted words\n",
    "    output :\n",
    "    counts_transposed # column transferred to rows\n",
    "    and saved to a csv\n",
    "    '''\n",
    "    counts_transposed = counts.T\n",
    "    counts_transposed['sum'] = counts_transposed.sum(axis=1)\n",
    "    cols = counts_transposed.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    counts_transposed = counts_transposed[cols]\n",
    "    counts_transposed = counts_transposed.sort_values(by=['sum'], axis=0, ascending=False)\n",
    "    movie_counts = counts_transposed\n",
    "    most_frequent_words = counts_transposed[['sum']]\n",
    "    most_frequent_words = most_frequent_words.reset_index()\n",
    "    most_frequent_words = most_frequent_words.rename(columns={'index': 'word', 'sum':'frequency'}, index={'title': 'index'})\n",
    "    return(most_frequent_words,counts_transposed)\n",
    "\n",
    "def cloud_word(word,nr_of_movies,movie_counts):\n",
    "    var = word\n",
    "    df = movie_counts.T[[var]]\n",
    "    return(df.sort_values(var, ascending=False).head(nr_of_movies))\n",
    "\n",
    "def make_database(freq_words,nr_of_movies,movie_counts):\n",
    "    word_list = freq_words['word']\n",
    "    word_list = word_list.tolist()\n",
    "    t = 0\n",
    "    for i in word_list:\n",
    "        movies = cloud_word(i,nr_of_movies,movie_counts)\n",
    "        movies = movies[1:]\n",
    "        movies = movies.reset_index()\n",
    "        movies = pd.DataFrame(data=movies)\n",
    "        movies = movies.rename(columns={'title':i})\n",
    "        movies = movies.iloc[:, :-1]\n",
    "        if t == 0:\n",
    "            df1 = movies\n",
    "        elif t> 1:\n",
    "            df1 = pd.concat([df1,movies],axis=1)\n",
    "        t+=1\n",
    "    return(df1)\n",
    "        \n",
    "def word_cloud(wrds):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    \n",
    "#     # iterate through the csv file \n",
    "#     for val in wrds: \n",
    "\n",
    "#         # typecaste each val to string \n",
    "#         val = str(val) \n",
    "\n",
    "#         # split the value \n",
    "#         tokens = val.split() \n",
    "\n",
    "#         # Converts each token into lowercase \n",
    "#         for i in range(len(tokens)): \n",
    "#             tokens[i] = tokens[i].lower() \n",
    "\n",
    "#         comment_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "    wordcloud = WordCloud(width = 500, height = 500, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords,\n",
    "                    random_state = None,\n",
    "                    min_font_size = 15).generate(','.join(wrds)) \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (10, 10), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('word_cloud_graphic.png')\n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selection types are : \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38000 entries, 0 to 37999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     38000 non-null  object\n",
      " 1   keywords  38000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 593.9+ KB\n",
      "None\n",
      "RAM = 52.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import requests\n",
    "import re\n",
    "import psutil\n",
    "#import openpyxl\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "'''\n",
    "Imports the 44,000 movie database into a pandas df so you can retrieve the columns.\n",
    "There are 24 columns in the database ranging from keywords to director.\n",
    "'''\n",
    "# Load the dataframe\n",
    "MAIN_FOLDER = 'c:/Users/champ/Python_proj/'\n",
    "MWML_FOLDER = 'made_with_ml_repo/temp/moviebuddy/meta_data/'\n",
    "FILE = \"movies.csv\"\n",
    "\n",
    "df = prep_db(MAIN_FOLDER + MWML_FOLDER + FILE,'csv')\n",
    "#df = df[['title', 'director', 'tagline', 'genres', 'keywords']]\n",
    "df = df[['title','keywords']]\n",
    "\n",
    "df = df.head(38000)\n",
    "df['keywords'] = df['keywords'].str.replace(', ', ',')\n",
    "df['keywords'] = df['keywords'].str.replace('\\s', '_')\n",
    "print('The selection types are : ')\n",
    "print(df.info())\n",
    "\n",
    "print(f'RAM = {psutil.virtual_memory().percent}')\n",
    "\n",
    "# Main functions\n",
    "counts = highest_count_words(df)\n",
    "frequent_words,movie_counts = most_frequent_words(counts)\n",
    "movies = make_database(freq_words = frequent_words.head(50),nr_of_movies = 10,movie_counts = movie_counts)\n",
    "movies.to_csv (MAIN_FOLDER + MWML_FOLDER + 'word_cloud_movie_recommendations.csv', index = False, header=True)\n",
    "\n",
    "# # This function cleans the database of unwanted words like \"the\" , \"an\" , other misc lanaguage characters, etc)  \n",
    "# counts = delete_columns_mega(counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A word cloud is generated here based on the occurance of keyword words\n",
    "'''\n",
    "#word_cloud(df['keywords'])\n",
    "word_cloud(df['keywords'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mach_learn_env",
   "language": "python",
   "name": "mach_learn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
